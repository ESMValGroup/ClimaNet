{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#spatio-temporal-model-architecture","title":"Spatio Temporal Model Architecture","text":"<p>The model takes daily SST (or similar) data in video format: <code>x \u2208 \u211d^{B \u00d7 1 \u00d7 T \u00d7 H \u00d7 W}</code> and a <code>daily_mask</code> indicating missing pixels. It also takes <code>land_mask_patch</code> indicating land regions in the output. The model does of the  following tasks:</p> <ul> <li>Combines video encoder, temporal attention, spatial transformer, and decoder</li> <li>Encodes 3D data (space, time) into spatio-temporal patches</li> <li>Aggregates temporal information per spatial patch</li> <li>Mixes spatial features across patches</li> <li>Decodes back to original spatial resolution</li> </ul> <p>The architecture consists of the following steps:</p> <pre><code># 1. Patch embedding:\nX (VideoEncoder)---------&gt; X_patch\n\n# 2. Add temporal encoding +\n# 3. Temporal aggregation:\nX_patch + PE (TemporalAttentionAggregator)---------&gt; X_temp_agg\n\n# 4. Add spatial encoding +\n# 5. Spatial transformer:\nX_temp_agg + PE (SpatialTransformer) ---------&gt; X_mixed\n\n# 6. Decode to original resolution:\nX_mixed (MonthlyConvDecoder)---------&gt; Output\n</code></pre>"},{"location":"#model-architecture-description","title":"Model architecture description","text":"<p>We explain the model architecture in more detail in the code and math description document.</p>"},{"location":"#references","title":"References","text":"<ul> <li>Attention is all you need</li> <li>VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</li> <li>Masked Autoencoders As Spatiotemporal Learners</li> <li>MAESSTRO: Masked Autoencoders for Sea Surface Temperature Reconstruction under Occlusion- 2024</li> </ul>"},{"location":"code_math_description/","title":"Spatio Temporal Model (class <code>SpatioTemporalModel</code>)","text":"<p>Summary: - Combines video encoder, temporal attention, spatial transformer, and decoder - Encodes video into spatio-temporal patches - Aggregates temporal information per spatial patch - Mixes spatial features across patches - Decodes back to original spatial resolution</p> <p>Detailed process:</p> <p>The model takes daily SST (or similar) data in video format: <code>x \u2208 \u211d^{B \u00d7 1 \u00d7 T \u00d7 H \u00d7 W}</code> and a <code>daily_mask</code> indicating missing pixels. It also takes <code>land_mask_patch</code> indicating land regions in the output.</p> <pre><code># 1. Patch embedding:\nX (VideoEncoder)---------&gt; X_patch\n\n# 2. Add temporal encoding +\n# 3. Temporal aggregation:\nX_patch + PE (TemporalAttentionAggregator)---------&gt; X_temp_agg\n\n# 4. Add spatial encoding +\n# 5. Spatial transformer:\nX_temp_agg + PE (SpatialTransformer) ---------&gt; X_mixed\n\n# 6. Decode to original resolution:\nX_mixed (MonthlyConvDecoder)---------&gt; Output\n</code></pre>"},{"location":"code_math_description/#1-video-encoder-class-videoencoder","title":"1. Video Encoder (class <code>VideoEncoder</code>)","text":"<p>Summary:</p> <ul> <li>Masked pixels are removed but their locations are preserved</li> <li>Video is split into 3D patches (time \u00d7 height \u00d7 width)</li> <li>Each patch becomes a vector (embedding)</li> <li>Output is a sequence suitable for Transformer-based video models (e.g. VideoMAE)</li> </ul> <p>Detailed process:</p> <p>We start with a video: Input video <code>x \u2208 R^{B \u00d7 1 \u00d7 T \u00d7 H \u00d7 W}</code> (batch, channel, time, height, width) and mask \u2208 <code>{0,1}^{B \u00d7 1 \u00d7 T \u00d7 H \u00d7 W}</code> where 1 (True) means missing / masked at ocean pixels. We define a validity indicator: <code>valid = 1 \u2212 mask</code>. So: valid = 1 \u2192 observed pixel, valid = 0 \u2192 missing pixel. We zero out missing values: <code>x_valid = x \u2299 valid</code>.</p> <p>We then concatenate the validity mask as a second channel: <code>x_cat = concat(x_valid, valid)</code>. Now the input has 2 channels: <code>x_cat \u2208 R^{B \u00d7 2 \u00d7 T \u00d7 H \u00d7 W}</code>. This allows the model to know which values were observed and which were missing.</p> <p>We split the video into non-overlapping spatio-temporal patches using a 3D convolution, see torch.nn.Conv3d. Let the patch size be: <code>(Pt, Ph, Pw)</code>. The convolution uses: <code>kernel size = (Pt, Ph, Pw)</code>, <code>stride = (Pt, Ph, Pw)</code>. This means each convolution output corresponds to one patch and patches do not overlap. Resulting shape: <code>z \u2208 R^{B \u00d7 D \u00d7 T' \u00d7 H' \u00d7 W'}</code> where: <code>D = embed_dim</code>, <code>T' = T / Pt, H' = H / Ph, W' = W / Pw</code>. Each <code>(t', h', w')</code> location is a patch embedding vector of length D.</p> <p>We flatten the 3D grid of patches into a sequence: <code>N = T' \u00d7 H' \u00d7 W'</code>. So each video becomes a sequence of patch embeddings, just like tokens in a Transformer.</p> <p>For each patch embedding, Layer normalization is done. This stabilizes training by normalizing across the embedding dimension, see torch.nn.LayerNorm.</p> <p>Randomly drops elements for regularization is done with Dropout. This helps prevent overfitting during training, see torch.nn.Dropout.</p> <p>The final output is: <code>{B \u00d7 N_patches \u00d7 embed_dim}</code>. Each element represents a spatio-temporal video patch, enriched with: visual information, knowledge of which pixels were valid or missing.</p>"},{"location":"code_math_description/#2-temporal-positional-encoding-class-temporalpositionalencoding","title":"2. Temporal positional encoding (class <code>TemporalPositionalEncoding</code>)","text":"<p>Summary:</p> <ul> <li>Each time step gets a unique vector</li> <li>Encodings are deterministic and fixed</li> <li>No learnable parameters</li> <li>Based on the Transformer positional encoding design</li> </ul> <p>Detailed process:</p> <p>The purpose of temporal positional encoding is to generate fixed temporal position vectors so that a model can know at which time index a feature occurs. The encoding depends only on time index, not on the data.</p> <p>Assume a temporal sequence of length: <code>T = 0, 1, 2, ..., T\u22121</code>. Each time index <code>t</code> is assigned a vector of length embedding dim. For time index <code>t</code> and embedding dimension index <code>i</code>: Even dimensions use sine, odd dimensions use cosine. This produces a unique, smooth encoding for each time step.</p> <p>For a maximum supported temporal length <code>max_len</code>, the class precomputes <code>pe</code> where row <code>t</code> contains the encoding for time index <code>t</code>. This matrix is fixed, not trainable and stored as a buffer. Later, in <code>forward</code> method, given a requested temporal length <code>T</code>, we have <code>output = PE[0:T]</code> and resulting shape is <code>(T, embed_dim)</code>. No parameters are learned and no computation depends on input data.</p>"},{"location":"code_math_description/#3-temporal-attention-aggregator-class-temporalattentionaggregator","title":"3. Temporal Attention Aggregator (class <code>TemporalAttentionAggregator</code>)","text":"<p>This is a temporal attention pooling over sequences of tokens.</p> <p>Summary:</p> <p>For each spatial patch (h, w):</p> <ul> <li>Collect its T temporal tokens: each spatial patch (h, w) has T temporal tokens</li> <li>Add temporal positional encoding for days within a month and for months within a year.</li> <li>Compute a learned scalar score per time step</li> <li>Apply softmax over time: it ensures the weights form a probability   distribution over time.</li> <li>Compute a weighted temporal sum: output is a temporal summary vector for each   patch, suitable for downstream tasks.</li> </ul> <p>Detailed process:</p> <p>We start with: <code>x \u2208 \u211d^{B \u00d7 (M.T\u00b7H\u00b7W) \u00d7 C}</code>, where <code>B</code> = batch size, <code>M</code>= number of months, <code>T</code> = number of temporal tokens per spatial patch, <code>H</code>, <code>W</code> = number of spatial patches along height and width, <code>C</code> = embedding dimension. We can reshape it to group by spatial patch where each spatial patch has its temporal sequence of length <code>T</code>.</p> <p>Then we add temporal positional encoding <code>pe</code> from <code>TemporalPositionalEncoding</code>. Add it to each temporal token <code>seq = seq + pe</code>. This injects time information (for days and for months) into each patch\u2019s token sequence.</p> <p>Then we compute temporal attention weights by applying <code>nn.Sequential</code> to get a scalar score, see torch.nn.Sequential. Here the explanation over each module in the sequential is as follows:</p> <ul> <li><code>LayerNorm</code> normalizes the features across the embedding dimension, which   helps stabilize training. This is a common practice before attention   computation.</li> <li><code>Linear(embed_dim, embed_dim)</code> learns which features are important for temporal weighting.</li> <li><code>GELU</code> allows learning non-linear relationships.</li> <li><code>Linear(embed_dim, 1)</code> projects to a single scalar score.</li> </ul> <p>see torch.nn for more details on each module.</p> <p>We apply the mask <code>padded_days_mask</code> where padded days (beyond the actual number of days in a month) are masked out. This ensures that the model does not attend to padded tokens that do not correspond to real data.Then, we convert scores into attention weights using softmax over time that represents importance of each temporal token for this patch. Then we aggregate temporal tokens by weighted sum over the temporal dimension. Result is one token per spatial patch.</p> <p>Then we apply cross month mixing using <code>nn.MultiheadAttention</code>. This allows tokens from different months to attend to each other, which can capture seasonal patterns. The attention is computed across the temporal dimension for each spatial patch. See torch.nn.MultiheadAttention for details on how multi-head attention works.</p> <p>Then we apply <code>nn.Sequential</code> again to the output of attention to get a final scalar score for each time step. The explanation for each module is the same as before: LayerNorm, Linear, GELU, Linear to 1 scalar. This gives us a score for each time step in the temporal sequence of each spatial patch.</p>"},{"location":"code_math_description/#4-spatial-positional-encoding-class-spatialpositionalencoding2d","title":"4. Spatial Positional Encoding (class <code>SpatialPositionalEncoding2D</code>)","text":"<p>Summary:</p> <ul> <li>Generate fixed sinusoidal positional encodings for 2D spatial grid</li> <li>Encodings are not learnable</li> <li>Intended to be added to spatial tokens</li> <li>Sine and cosine functions of different frequencies allow the model to   distinguish positions along height and width.</li> </ul> <p>Detailed process:</p> <p>The module generates fixed 2D positional encodings for a grid of size (H, W) and embedding dimension embed_dim. Each spatial location (h, w) gets a unique vector of length embed_dim. Encodings are deterministic (not learned) and based on sine/cosine functions, similar to the <code>Temporal positional encoding</code>. The encoding for each spatial location is a combination of sine and cosine functions of different frequencies along height and width. This allows the model to know the spatial position of each token when added to the spatial tokens.</p>"},{"location":"code_math_description/#5-spatial-transformer-class-spatialtransformer","title":"5. Spatial Transformer (class <code>SpatialTransformer</code>)","text":"<p>It mixes spatial patch tokens using multi-head self-attention.</p> <p>Summary:</p> <ul> <li>Applies a standard Transformer encoder to spatial tokens</li> <li>Mixes information across spatial locations: each patch token can attend to all   other patches, allowing global spatial context</li> <li>Output is a sequence of spatially mixed tokens of the same shape as input</li> </ul> <p>Detailed process:</p> <p>We apply a single Transformer encoder block using <code>nn.TransformerEncoderLayer</code> that performs self-attention and feedforward processing. In Multi-Head Self-Attention, Every token looks at every other token and with <code>num_heads=4</code>, this happens in 4 parallel \"perspectives.\" In Feedforward Network (MLP), each token is processed independently through a small neural network to allow complex feature interactions. This is useful for spatial data and allows:</p> <ul> <li>Global context: Every patch can \"see\" every other patch</li> <li>Spatial mixing: Information flows across the entire image</li> <li>Learning relationships: Model learns which patches are relevant to each other</li> </ul> <p>Then, <code>nn.TransformerEncoder</code> stacks multiple encoder layers sequentially; it's a container that repeats the same transformer block <code>depth</code> times.</p>"},{"location":"code_math_description/#6-monthly-convolution-decoder-class-monthlyconvdecoder","title":"6. Monthly Convolution Decoder (class <code>MonthlyConvDecoder</code>)","text":"<p>Summary:</p> <ul> <li>Reshape latent tokens</li> <li>Apply 1x1 convolution to mix features</li> <li>Use transposed convolution to upsample to original spatial size</li> <li>Applies a convolutional refinement block to smooth patch boundaries.</li> <li>Apply convolutional head for final output</li> <li>Optionally mask out land regions</li> <li>Add scale and bias to output</li> </ul> <p>Detailed process:</p> <p>We transforms the embedding dimension (default embed_dim=128) to the hidden dimension (default hidden=128) at each spatial location independently by a 1\u00d71 convolution (also called a \"pointwise convolution\") using <code>nn.Conv2d(..., kernel_size=1)</code>. Even though both input and output dimensions are 128 by default, this layer learns a linear transformation to mix and re-weight the channel features.</p> <p>Then we use a deconvolution (<code>ConvTranspose2d</code>) to map each patch to its original pixel grid. It converts the low-resolution patch grid back to the original image resolution. The padding is set to the overlap, which allows for smooth upsampling and some overlap between patches.</p> <p>Then we apply a convolutional refinement block to smooth out any artifacts from the deconvolution. Deconvolution can sometimes produce blocky outputs, so this refinement helps to smooth patch boundaries and improve spatial coherence. This block consists of a <code>nn.Sequential</code>. This processing block contains:</p> <ul> <li><code>Conv2d</code>: Convolution to mix features and reduce artifacts.</li> <li><code>BatchNorm2d</code>: Normalizes the features to stabilize training.</li> <li><code>GELU</code>: Non-linear activation to allow complex feature interactions.</li> <li><code>Conv2d(1\u00d71)</code>: Final 1\u00d71 convolution for deeper feature refinement.</li> <li><code>BatchNorm2d</code>: Re-normalizes the features to stabilize training.</li> <li><code>GELU</code>: Another non-linear activation to allow complex feature interactions.</li> </ul> <p>A single <code>Conv2d(..., 1, kernel_size=1)</code> could work, but the extra layers provide:</p> <ul> <li>Spatial refinement after deconvolution (which can produce artifacts)</li> <li>Non-linearity for more expressive power</li> <li>Better gradient flow during training</li> </ul> <p>Finally, we apply a small convolutional head to produce the final single-channel output. Then we apply scale, bias, and (optional) land mask to get the final output (he reconstructed 2D map (e.g., SST) for each batch).</p>"},{"location":"code_math_description/#references","title":"References","text":"<ul> <li>Attention is all you need</li> <li>VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</li> <li>Masked Autoencoders As Spatiotemporal Learners</li> <li>MAESSTRO: Masked Autoencoders for Sea Surface Temperature Reconstruction under Occlusion- 2024</li> </ul>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#prediction-of-monthly-sst","title":"Prediction of Monthly SST","text":"<p>We provide an example of how to use the <code>st_encoder_decoder</code> module to predict monthly sea surface temperature (SST) using a spatio-temporal encoder-decoder architecture. The example is implemented in a Jupyter notebook, which can be found in the <code>notebooks</code> directory.</p>"},{"location":"api/st_encoder_decoder/","title":"st_encoder_decoder.py","text":"<p>Spatio-Temporal encoder-decoder for Monthly Prediction. The main model class is SpatioTemporalModel.</p>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.MonthlyConvDecoder","title":"<code>MonthlyConvDecoder(embed_dim=128, patch_h=4, patch_w=4, hidden=128, overlap=1, num_months=12)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Decoder to reconstruct 2D maps from patch tokens.</p> <p>The MonthlyConvDecoder converts latent patch tokens back to pixel space:     - Applies a 1*1 convolution to mix features on the patch grid.     - Uses a transposed convolution (deconvolution) to upsample tokens to the original spatial resolution.     - Applies a convolutional refinement block to smooth patch boundaries.     - Applies a small convolutional head to produce the final single-channel output.     - Optionally masks out land regions using a boolean mask.</p> <p>Args:     embed_dim: Dimension of the patch embedding.The default is 128.         Many vision transformers use embedding dimensions that are         multiples of 64 (e.g., 64, 128, 256). This can be tuned.     patch_h: Patch height     patch_w: Patch width     hidden: Hidden dimension in the decoder for mixing channel features.         The default is 128, which can be tuned.     overlap: Overlap size for deconvolution. It creates smooth blending         between adjacent upsampled patches. Default is 1, no overlap at edges.     num_months: Number of months. Default is 12.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(\n    self, embed_dim=128, patch_h=4, patch_w=4, hidden=128, overlap=1, num_months=12\n):\n    \"\"\"\n    Args:\n        embed_dim: Dimension of the patch embedding.The default is 128.\n            Many vision transformers use embedding dimensions that are\n            multiples of 64 (e.g., 64, 128, 256). This can be tuned.\n        patch_h: Patch height\n        patch_w: Patch width\n        hidden: Hidden dimension in the decoder for mixing channel features.\n            The default is 128, which can be tuned.\n        overlap: Overlap size for deconvolution. It creates smooth blending\n            between adjacent upsampled patches. Default is 1, no overlap at edges.\n        num_months: Number of months. Default is 12.\n    \"\"\"\n    super().__init__()\n    self.patch_h = patch_h\n    self.patch_w = patch_w\n    self.overlap = overlap\n\n    # Mix channel features on the patch grid (Hp, Wp)\n    # Input shape: (B, embed_dim, Hp, Wp) \u2192 Output shape: (B, hidden, Hp, Wp)\n    # here kernel_size=1 means we are mixing features at each patch location\n    # without spatial interaction\n    in_channels, out_channels = embed_dim, hidden\n    self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    # Upsample to full resolution\n    # With kernel = stride + 2*overlap and padding=overlap,\n    # output size is exact: H = Hp*patch_h, W = Wp*patch_w (no output_padding needed).\n    k_h = patch_h + 2 * overlap\n    k_w = patch_w + 2 * overlap\n    # As spatial size increases, channel count decreases to keep computation\n    # manageable; here  hidden // 2 is a design choice.\n    in_channels, out_channels = hidden, hidden // 2\n    self.deconv = nn.ConvTranspose2d(\n        in_channels,\n        out_channels,\n        kernel_size=(k_h, k_w),\n        stride=(patch_h, patch_w),\n        padding=overlap,\n        output_padding=0,\n        bias=True,\n    )\n\n    # Final conv head to get single channel output kernel_size=3 is the most\n    # common choice for spatial convolutions; it's the smallest kernel that\n    # captures spatial context in all directions\n    in_channels, out_channels = hidden // 2, hidden // 2\n\n    # Refinement block: a small conv layers to smooth patch boundaries\n    self.refine = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.GELU(),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.GELU(),\n    )\n\n    # Final conv head to map to single-channel output\n    self.head = nn.Conv2d(out_channels, 1, kernel_size=1)\n\n    # Learnable scale and bias (mean and std) to improve predictions\n    self.scale = nn.Parameter(torch.ones(num_months))\n    self.bias = nn.Parameter(torch.zeros(num_months))\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.MonthlyConvDecoder.forward","title":"<code>forward(latent, M, out_H, out_W, land_mask=None)</code>","text":"<p>Reconstruct 2D maps from latent patch tokens. Args:     latent: Tensor of shape (B, MHpWp, C) where C is the embedding dimension.     M: Number of months (temporal patches)     out_H: Target output height (must be divisible by patch_h)     out_W: Target output width (must be divisible by patch_w)     land_mask: Optional boolean tensor of shape (out_H, out_W). Values set to True         will be masked out (set to 0) in the output (only ocean pixels exist). Returns:     Tensor of shape (B, M, out_H, out_W) representing the monthly variable e.g. SST.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, latent, M, out_H, out_W, land_mask=None):\n    \"\"\"Reconstruct 2D maps from latent patch tokens.\n    Args:\n        latent: Tensor of shape (B, M*Hp*Wp, C) where C is the embedding dimension.\n        M: Number of months (temporal patches)\n        out_H: Target output height (must be divisible by patch_h)\n        out_W: Target output width (must be divisible by patch_w)\n        land_mask: Optional boolean tensor of shape (out_H, out_W). Values set to True\n            will be masked out (set to 0) in the output (only ocean pixels exist).\n    Returns:\n        Tensor of shape (B, M, out_H, out_W) representing the monthly variable e.g. SST.\n    \"\"\"\n    B, MHW, C = latent.shape\n    Hp = out_H // self.patch_h\n    Wp = out_W // self.patch_w\n    assert MHW == M * Hp * Wp, f\"Token mismatch: got {MHW}, expected {M * Hp * Wp}\"\n\n    # transforms the latent tensor from sequence format to image format for\n    # convolution operations; (B, M*Hp*Wp, C) -&gt; (B*M, C, Hp, Wp)\n    out = latent.view(B, M, Hp, Wp, C).permute(0, 1, 4, 2, 3).contiguous()\n    out = out.view(B * M, C, Hp, Wp)\n\n    # Apply 1x1 convolution to mix features\n    out = self.proj(out)  # (B*M, hidden, Hp, Wp)\n\n    # Use transposed convolution to upsample\n    out = self.deconv(out)  # (B*M, hidden//2, H, W)\n\n    # Refinement CNN to smooth boundaries\n    out = self.refine(out)  # (B*M, hidden//2, H, W)\n\n    # Apply final conv head to get single channel output\n    out = self.head(out)  # (B*M, 1, H, W)\n\n    # Apply scale and bias per month to improve predictions; reshape to (B*M, 1, 1, 1) for broadcasting\n    scale = self.scale[:M].unsqueeze(0).expand(B, M).reshape(B * M, 1, 1, 1)\n    bias = self.bias[:M].unsqueeze(0).expand(B, M).reshape(B * M, 1, 1, 1)\n    out = out * scale + bias\n    out = out.view(B, M, out_H, out_W)  # (B, M, H, W)\n\n    # Mask out land areas if land_mask is provided\n    if land_mask is not None:\n        out = out.masked_fill(land_mask.bool()[None, None, :, :], 0.0)\n    return out  # (B, M, out_H, out_W)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatialPositionalEncoding2D","title":"<code>SpatialPositionalEncoding2D(embed_dim=128, max_H=1024, max_W=1024)</code>","text":"<p>               Bases: <code>Module</code></p> <p>2D Spatial Positional Encoding using sine and cosine functions.</p> <p>This module generates fixed sinusoidal positional encodings for a 2D spatial grid, following the formulation in \"Attention Is All You Need\" (Vaswani et al., 2017).</p> <p>The returned positional encodings are intended to be added to spatial tokens by the caller. The encodings are not learnable.</p> <p>Initialize the positional encoding. Args:     embed_dim: Dimension of the embedding, it must be even. The default is 128.         Embedding dimensions are usually multiples of 64 (e.g., 64, 128,         256). This can be tuned.     max_H: Maximum height. Default is 1024, which should be sufficient.     max_W: Maximum width. Default is 1024, which should be sufficient.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(self, embed_dim=128, max_H=1024, max_W=1024):\n    \"\"\"Initialize the positional encoding.\n    Args:\n        embed_dim: Dimension of the embedding, it must be even. The default is 128.\n            Embedding dimensions are usually multiples of 64 (e.g., 64, 128,\n            256). This can be tuned.\n        max_H: Maximum height. Default is 1024, which should be sufficient.\n        max_W: Maximum width. Default is 1024, which should be sufficient.\n    \"\"\"\n    super().__init__()\n    self.embed_dim = embed_dim\n    self.max_H = max_H\n    self.max_W = max_W\n    self.register_buffer(\n        \"pe\", self.build_pe(max_H, max_W, embed_dim), persistent=False\n    )\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatialPositionalEncoding2D.build_pe","title":"<code>build_pe(H, W, embed_dim)</code>  <code>staticmethod</code>","text":"<p>Build the 2D positional encoding encoding tensor. Args:     H: Height of the grid     W: Width of the grid     embed_dim: Dimension of the embedding (must be even) Returns:     Tensor of shape (H, W, embed_dim) containing fixed positional encodings.     Encodings are constructed by combining sine/cosine encodings along     height and width. Not learnable.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>@staticmethod\ndef build_pe(H, W, embed_dim):\n    \"\"\"Build the 2D positional encoding encoding tensor.\n    Args:\n        H: Height of the grid\n        W: Width of the grid\n        embed_dim: Dimension of the embedding (must be even)\n    Returns:\n        Tensor of shape (H, W, embed_dim) containing fixed positional encodings.\n        Encodings are constructed by combining sine/cosine encodings along\n        height and width. Not learnable.\n    \"\"\"\n    assert embed_dim % 2 == 0, \"embed_dim must be even\"\n    pe_h = torch.zeros(H, embed_dim // 2)\n    pe_w = torch.zeros(W, embed_dim // 2)\n    pos_h = torch.arange(H).unsqueeze(1)\n    pos_w = torch.arange(W).unsqueeze(1)\n    div = torch.exp(\n        torch.arange(0, embed_dim // 2, 2) * (-math.log(10000.0) / (embed_dim // 2))\n    )\n    pe_h[:, 0::2] = torch.sin(pos_h * div)\n    pe_h[:, 1::2] = torch.cos(pos_h * div)\n    pe_w[:, 0::2] = torch.sin(pos_w * div)\n    pe_w[:, 1::2] = torch.cos(pos_w * div)\n    pe_2d = pe_h.unsqueeze(1) + pe_w.unsqueeze(0)  # (H, W, embed_dim/2)\n    # concatenate to reach embed_dim\n    pe = torch.cat([pe_2d, pe_2d], dim=-1)  # (H, W, embed_dim)\n    return pe  # not learned\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatialPositionalEncoding2D.forward","title":"<code>forward(Hp, Wp)</code>","text":"<p>Get positional encoding for size (Hp, Wp). Args:     Hp: Height after patching (\u2264 max_H)     Wp: Width after patching (\u2264 max_W) Returns:     Tensor of shape (Hp*Wp, embed_dim) containing positional encodings     flattened in row-major order (height * width).</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, Hp, Wp):\n    \"\"\"Get positional encoding for size (Hp, Wp).\n    Args:\n        Hp: Height after patching (\u2264 max_H)\n        Wp: Width after patching (\u2264 max_W)\n    Returns:\n        Tensor of shape (Hp*Wp, embed_dim) containing positional encodings\n        flattened in row-major order (height * width).\n    \"\"\"\n    # returns (Hp*Wp, embed_dim)\n    pe_hw = self.pe[:Hp, :Wp, :].reshape(Hp * Wp, -1)\n    return pe_hw\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatialTransformer","title":"<code>SpatialTransformer(embed_dim=128, depth=2, num_heads=4, mlp_ratio=4.0, dropout=0.0)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatial Transformer for spatial feature mixing.</p> <p>This module applies a standard Transformer encoder to a sequence of spatial tokens (patch embeddings), allowing information to be mixed across all spatial locations.</p> <p>Key points:     - Uses multi-head self-attention and feedforward layers.     - Designed to operate on flattened spatial tokens.</p> <p>Initialize the spatial transformer. Args:     embed_dim: Dimension of the embedding. Default is 128.         The embedding dimensions are multiples of 64 (e.g., 64, 128,         256). This can be tuned.     depth: Number of transformer encoder layers. Default is 2. This can be         increased for more complex spatial mixing.     num_heads: Number of attention heads in each layer. Default is 4.         When embed_dim is 128, 4 heads is a common choice.     mlp_ratio: Ratio of feedforward hidden dimension to embed_dim. Default is 4.0.     dropout: Dropout rate applied to attention and feedforward layers. Default is 0.0.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(self, embed_dim=128, depth=2, num_heads=4, mlp_ratio=4.0, dropout=0.0):\n    \"\"\"Initialize the spatial transformer.\n    Args:\n        embed_dim: Dimension of the embedding. Default is 128.\n            The embedding dimensions are multiples of 64 (e.g., 64, 128,\n            256). This can be tuned.\n        depth: Number of transformer encoder layers. Default is 2. This can be\n            increased for more complex spatial mixing.\n        num_heads: Number of attention heads in each layer. Default is 4.\n            When embed_dim is 128, 4 heads is a common choice.\n        mlp_ratio: Ratio of feedforward hidden dimension to embed_dim. Default is 4.0.\n        dropout: Dropout rate applied to attention and feedforward layers. Default is 0.0.\n    \"\"\"\n    super().__init__()\n\n    # a single Transformer encoder block that\n    # performs self-attention and feedforward processing\n    encoder_layer = nn.TransformerEncoderLayer(\n        d_model=embed_dim,\n        nhead=num_heads,\n        dim_feedforward=int(embed_dim * mlp_ratio),\n        batch_first=True,\n        dropout=dropout,\n        activation=\"gelu\",\n    )\n    # stack multiple layers to form the full encoder\n    self.enc = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatialTransformer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the spatial transformer. Args:     x: Input tensor of shape (B, N, C), where N = number of spatial tokens (H'*W') and         C = embedding dimension Returns:     Tensor of shape (B, N, C) with spatially mixed features across patches</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the spatial transformer.\n    Args:\n        x: Input tensor of shape (B, N, C), where N = number of spatial tokens (H'*W') and\n            C = embedding dimension\n    Returns:\n        Tensor of shape (B, N, C) with spatially mixed features across patches\n    \"\"\"\n    return self.enc(x)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatioTemporalModel","title":"<code>SpatioTemporalModel(in_chans=1, embed_dim=128, patch_size=(1, 4, 4), max_days=31, max_months=12, hidden=128, overlap=1, max_H=1024, max_W=1024, spatial_depth=2, spatial_heads=4)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatio-Temporal Model for Monthly Prediction.</p> <p>Processes daily data in a video-style format with shape (B, C, T, H, W):     B: batch size     C: number of channels (e.g., 1 for SST, can include additional channels like masks)     T: temporal dimension (number of days, e.g., 31 for a month)     H: spatial height     W: spatial width</p> <p>The model pipeline:     1. Encode spatio-temporal patches using VideoEncoder.     2. Aggregate temporal information for each spatial patch via TemporalAttentionAggregator.     3. Add 2D spatial positional encodings and mix spatial features with SpatialTransformer.     4. Decode aggregated tokens into a full-resolution 2D map using MonthlyConvDecoder.</p> <p>Output:     - Reconstructed monthly (SST) map of shape (B, M, H, W)</p> <p>Initialize the Spatio-Temporal Model.</p> <p>Args:     in_chans: Number of input channels (e.g., 1 for SST, additional channels possible)     embed_dim: Dimension of the patch embedding     patch_size: Tuple of (T, H, W) patch sizes for temporal and spatial patching     max_days: Maximum number of days for temporal positional encoding     max_months: Maximum number of months for temporal positional encoding     hidden: Hidden dimension used in the decoder     overlap: Overlap for deconvolution in the decoder     max_H: Maximum spatial height for 2D positional encoding     max_W: Maximum spatial width for 2D positional encoding     spatial_depth: Number of layers in the spatial Transformer     spatial_heads: Number of attention heads in the spatial Transformer</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(\n    self,\n    in_chans=1,\n    embed_dim=128,\n    patch_size=(1, 4, 4),\n    max_days=31,\n    max_months=12,\n    hidden=128,\n    overlap=1,\n    max_H=1024,\n    max_W=1024,\n    spatial_depth=2,\n    spatial_heads=4,\n):\n    \"\"\"Initialize the Spatio-Temporal Model.\n\n    Args:\n        in_chans: Number of input channels (e.g., 1 for SST, additional channels possible)\n        embed_dim: Dimension of the patch embedding\n        patch_size: Tuple of (T, H, W) patch sizes for temporal and spatial patching\n        max_days: Maximum number of days for temporal positional encoding\n        max_months: Maximum number of months for temporal positional encoding\n        hidden: Hidden dimension used in the decoder\n        overlap: Overlap for deconvolution in the decoder\n        max_H: Maximum spatial height for 2D positional encoding\n        max_W: Maximum spatial width for 2D positional encoding\n        spatial_depth: Number of layers in the spatial Transformer\n        spatial_heads: Number of attention heads in the spatial Transformer\n    \"\"\"\n    super().__init__()\n    self.encoder = VideoEncoder(\n        in_chans=in_chans, embed_dim=embed_dim, patch_size=patch_size\n    )\n    self.temporal = TemporalAttentionAggregator(\n        embed_dim=embed_dim, max_days=max_days, max_months=max_months\n    )\n    self.spatial_pe = SpatialPositionalEncoding2D(\n        embed_dim=embed_dim, max_H=max_H, max_W=max_W\n    )\n    self.spatial_tr = SpatialTransformer(\n        embed_dim=embed_dim, depth=spatial_depth, num_heads=spatial_heads\n    )\n    self.decoder = MonthlyConvDecoder(\n        embed_dim=embed_dim,\n        patch_h=patch_size[1],\n        patch_w=patch_size[2],\n        hidden=hidden,\n        overlap=overlap,\n        num_months=max_months,\n    )\n    self.patch_size = patch_size\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.SpatioTemporalModel.forward","title":"<code>forward(daily_data, daily_mask, land_mask_patch, padded_days_mask=None)</code>","text":"<p>Forward pass of the Spatio-Temporal model.</p> <p>Args:     daily_data: Tensor of shape (B, C, M, T, H, W) containing daily         data, where C is the number of channels (e.g., 1 for SST)     daily_mask: Boolean tensor of same shape as daily_data indicating missing values     land_mask_patch: Boolean tensor of shape (H, W) to mask land areas in the output     padded_days_mask: Optional boolean tensor of shape (B, M, T) indicating which day tokens are padded          (True for padded tokens). Used to mask out padded tokens in temporal attention. Returns:     monthly_pred: Tensor of shape (B, M, H, W) representing the reconstructed monthly map</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, daily_data, daily_mask, land_mask_patch, padded_days_mask=None):\n    \"\"\"Forward pass of the Spatio-Temporal model.\n\n    Args:\n        daily_data: Tensor of shape (B, C, M, T, H, W) containing daily\n            data, where C is the number of channels (e.g., 1 for SST)\n        daily_mask: Boolean tensor of same shape as daily_data indicating missing values\n        land_mask_patch: Boolean tensor of shape (H, W) to mask land areas in the output\n        padded_days_mask: Optional boolean tensor of shape (B, M, T) indicating which day tokens are padded\n             (True for padded tokens). Used to mask out padded tokens in temporal attention.\n    Returns:\n        monthly_pred: Tensor of shape (B, M, H, W) representing the reconstructed monthly map\n    \"\"\"\n    B, C, M, T, H, W = daily_data.shape\n\n    Tp = T // self.patch_size[0]\n    Hp = H // self.patch_size[1]\n    Wp = W // self.patch_size[2]\n    Np = Tp * Hp * Wp\n\n    # check shape and patch compatibility\n    assert daily_mask.shape == daily_data.shape, (\n        \"daily_mask must have the same shape as daily_data\"\n    )\n    assert H % self.patch_size[1] == 0 and W % self.patch_size[2] == 0, (\n        \"H and W must be divisible by patch size\"\n    )\n    assert T % self.patch_size[0] == 0, \"T must be divisible by patch size\"\n\n    # Step 1: Encode spatio-temporal patches\n    # each month independently by folding M into batch\n    daily_data_reshaped = daily_data.reshape(B * M, C, T, H, W)\n    daily_mask_reshaped = daily_mask.reshape(B * M, C, T, H, W)\n    latent = self.encoder(\n        daily_data_reshaped, daily_mask_reshaped\n    )  # (B*M, N_patches, embed_dim)\n\n    # Step 2: Aggregate temporal information for each spatial patch\n    # latent -&gt; (B, M*Np, embed_dim) to match the aggregator input x: (B, M*Tp*Hp*Wp, embed_dim)\n    latent = latent.reshape(B, M * Np, -1)\n\n    if padded_days_mask is not None and self.patch_size[0] &gt; 1:\n        B, M, T_days = padded_days_mask.shape\n        if T_days % self.patch_size[0] != 0:\n            raise ValueError(\n                f\"T_days={T_days} must be divisible by patch_size[0]={self.patch_size[0]}\"\n            )\n        padded_days_mask = padded_days_mask.view(\n            B, M, T_days // self.patch_size[0], self.patch_size[0]\n        ).all(dim=-1)  # (B, M, Tp)\n\n    agg_latent = self.temporal(\n        latent, M, Tp, Hp, Wp, padded_days_mask=padded_days_mask\n    )  # (B, M*Hp*Wp, embed_dim)\n\n    # Step 3: Add spatial positional encodings and mix spatial features\n    E = agg_latent.shape[-1]\n    agg_latent = agg_latent.view(B, M, Hp * Wp, E)\n    pe = (\n        self.spatial_pe(Hp, Wp).to(agg_latent.device).to(agg_latent.dtype)\n    )  # (Hp*Wp, E)\n    x = agg_latent + pe[None, None, :, :]\n\n    # Step 4: Spatial mixing with Transformer\n    x = x.view(B * M, Hp * Wp, E)\n    x = self.spatial_tr(x)  # (B*M, Hp*Wp, E)\n    x = x.view(B, M * Hp * Wp, E)  # back to (B, M*Hp*Wp, E)\n\n    # Step 5: Decode to full-resolution 2D map\n    monthly_pred = self.decoder(x, M, H, W, land_mask_patch)  # (B, M, H, W)\n    return monthly_pred\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.TemporalAttentionAggregator","title":"<code>TemporalAttentionAggregator(embed_dim=128, max_days=31, max_months=12)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Temporal attention-based aggregator.</p> <p>This module aggregates temporal information for each spatial patch by applying attention across the temporal dimension. It consists of two main steps: 1. Day attention: For each month, it computes attention weights across the temporal tokens (days) and performs a weighted sum to get one token per spatial location for each month. 2. Cross-month mixing: After temporal aggregation, it applies a Transformer encoder layer to mix information across months at each spatial location.</p> <p>For each spatial location, the day attention allows the model to learn which days are most important for predicting the monthly average, while the cross-month mixing allows the model to learn interactions between different months.</p> <p>Initialize the temporal attention aggregator.</p> <p>Args:     embed_dim: Dimension of the embedding. The default is 128.         Many vision transformers use embedding dimensions that are multiples         of 64 (e.g., 64, 128, 256). This can be tuned.     max_days: Maximum length of the temporal dimension to precompute     encodings for. Default is 31, which is sufficient for a month of     daily data.     max_months: Maximum number of months (temporal patches) to precompute     encodings for. Default is 12, which is sufficient for a year of monthly data.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(self, embed_dim=128, max_days=31, max_months=12):\n    \"\"\"Initialize the temporal attention aggregator.\n\n    Args:\n        embed_dim: Dimension of the embedding. The default is 128.\n            Many vision transformers use embedding dimensions that are multiples\n            of 64 (e.g., 64, 128, 256). This can be tuned.\n        max_days: Maximum length of the temporal dimension to precompute\n        encodings for. Default is 31, which is sufficient for a month of\n        daily data.\n        max_months: Maximum number of months (temporal patches) to precompute\n        encodings for. Default is 12, which is sufficient for a year of monthly data.\n    \"\"\"\n    super().__init__()\n\n    # Positional encodings for days and months\n    self.pos_days = TemporalPositionalEncoding(embed_dim, max_len=max_days)\n    self.pos_months = TemporalPositionalEncoding(embed_dim, max_len=max_months)\n\n    # Day scorer (within each month)\n    self.day_scorer = nn.Sequential(\n        nn.LayerNorm(embed_dim),  # normalizing features\n        nn.Linear(embed_dim, embed_dim),  # learns temporal feature transformation\n        nn.GELU(),  # adds non-linearity to capture complex temporal patterns\n        nn.Linear(embed_dim, 1),  # project to a single score\n    )\n\n    # Cross month mixing\n    self.month_ln = nn.LayerNorm(embed_dim)\n    self.month_attn = nn.MultiheadAttention(\n        embed_dim=embed_dim,\n        num_heads=4,\n        dropout=0.0,\n        batch_first=True,\n    )\n    self.month_ffn = nn.Sequential(\n        nn.LayerNorm(embed_dim),\n        nn.Linear(\n            embed_dim, 4 * embed_dim\n        ),  # 4 is a common factor in transformer feedforward layers\n        nn.GELU(),\n        nn.Linear(4 * embed_dim, embed_dim),\n    )\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.TemporalAttentionAggregator.forward","title":"<code>forward(x, M, T, H, W, padded_days_mask=None)</code>","text":"<p>Args:     x: (B, MTHW, C) containing spatio-temporal tokens, where C is the embedding dimension.     M: number of months     T: number of temporal tokens per month after temporal patching (Tp)     H: spatial height after spatial patching     W: spatial width after spatial patching     padded_days_mask: Optional boolean tensor of shape (B, M, T), bool,         True indicating which day tokens are padded (because some months         have fewer days). This is used to mask out padded tokens in attention computation. Returns:     Tensor of shape (B, MH*W, C) with one temporally aggregated, where C is the embedding dimension.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, x, M, T, H, W, padded_days_mask=None):\n    \"\"\"\n    Args:\n        x: (B, M*T*H*W, C) containing spatio-temporal tokens, where C is the embedding dimension.\n        M: number of months\n        T: number of temporal tokens per month after temporal patching (Tp)\n        H: spatial height after spatial patching\n        W: spatial width after spatial patching\n        padded_days_mask: Optional boolean tensor of shape (B, M, T), bool,\n            True indicating which day tokens are padded (because some months\n            have fewer days). This is used to mask out padded tokens in attention computation.\n    Returns:\n        Tensor of shape (B, M*H*W, C) with one temporally aggregated, where C is the embedding dimension.\n    \"\"\"\n    seq = rearrange(x, \"b (m t h w) c -&gt; b (h w) m t c\", m=M, t=T, h=H, w=W)\n\n    pe_days = self.pos_days(T).to(seq.device).to(seq.dtype)  # (T, C)\n    pe_months = self.pos_months(M).to(seq.device).to(seq.dtype)  # (M, C)\n\n    seq = seq + pe_days[None, None, None, :, :]  # add day PE\n    seq = seq + pe_months[None, None, :, None, :]  # add month PE\n\n    # Day attention per month\n    day_logits = self.day_scorer(seq).squeeze(-1)  # (B, HW, M, T)\n\n    # padded_days_mask is (B, M, T) true=padded, -&gt; (B, HW, M, T)\n    if padded_days_mask is not None:\n        pad = padded_days_mask[:, None, :, :].expand(x.shape[0], H * W, M, T)\n        day_logits = day_logits.masked_fill(pad, float(\"-inf\"))\n\n    day_w = torch.softmax(day_logits, dim=-1)\n    month_tokens = (seq * day_w.unsqueeze(-1)).sum(dim=3)  # (B, HW, M, C)\n\n    # Cross-month attention at each spatial location\n    z = rearrange(month_tokens, \"b s m c -&gt; (b s) m c\")\n    z_ln = self.month_ln(z)\n    attn_out, _ = self.month_attn(z_ln, z_ln, z_ln, need_weights=False)\n    z = z + attn_out\n    z = z + self.month_ffn(z)\n\n    # Back to flattened tokens with month kept\n    z = rearrange(z, \"(b s) m c -&gt; b s m c\", b=x.shape[0], s=H * W)\n    out = rearrange(z, \"b (h w) m c -&gt; b (m h w) c\", h=H, w=W)\n    return out  # (B, M*H*W, C)  C: embedding dimension\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.TemporalPositionalEncoding","title":"<code>TemporalPositionalEncoding(embed_dim=128, max_len=31)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Temporal Positional Encoding using sine and cosine functions.</p> <p>This module generates fixed (non-learnable) sinusoidal positional encodings for the temporal dimension, following the formulation in \"Attention Is All You Need\" (Vaswani et al., 2017).</p> <p>The returned positional encodings are intended to be added to temporal embeddings by the caller, but this module itself does not perform the addition.</p> <p>Initialize the temporal positional encoding. Args:     embed_dim: Dimension of the embedding.The default is 128.         Many vision transformers use embedding dimensions that are multiples         of 64 (e.g., 64, 128, 256). This can be tuned.     max_len: Maximum length of the temporal dimension to precompute     encodings for. Default is 31, which is sufficient for a month of     daily data.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(self, embed_dim=128, max_len=31):\n    \"\"\"Initialize the temporal positional encoding.\n    Args:\n        embed_dim: Dimension of the embedding.The default is 128.\n            Many vision transformers use embedding dimensions that are multiples\n            of 64 (e.g., 64, 128, 256). This can be tuned.\n        max_len: Maximum length of the temporal dimension to precompute\n        encodings for. Default is 31, which is sufficient for a month of\n        daily data.\n    \"\"\"\n    super().__init__()\n    pe = torch.zeros(max_len, embed_dim)\n    position = torch.arange(0, max_len).unsqueeze(1)\n    div_term = torch.exp(\n        torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim)\n    )\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    self.register_buffer(\"pe\", pe)  # (max_len, embeddim)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.TemporalPositionalEncoding.forward","title":"<code>forward(T)</code>","text":"<p>Return positional encodings for a temporal sequence. Args:     T: Temporal length (must be &lt;= max_len) Returns:     Tensor of shape (T, embed_dim) containing sinusoidal positional encodings</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, T):\n    \"\"\"Return positional encodings for a temporal sequence.\n    Args:\n        T: Temporal length (must be &lt;= max_len)\n    Returns:\n        Tensor of shape (T, embed_dim) containing sinusoidal positional encodings\n    \"\"\"\n    return self.pe[:T]  # (T, embed_dim)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.VideoEncoder","title":"<code>VideoEncoder(in_chans=1, embed_dim=128, patch_size=(1, 4, 4), drop=0.0)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Video Encoder with spatio-temporal patch embedding.</p> <p>This module converts an input video into a sequence of non-overlapping spatio-temporal patch embeddings using a 3D convolution.</p> <p>Masking is handled by: - zeroing out masked (missing) pixels - concatenating a validity mask as an additional input channel</p> <p>The convolution uses kernel size and stride equal to the patch size. The output is a sequence of patch embeddings, as used in VideoMAE: https://arxiv.org/abs/2203.12602</p> <p>Args:     in_chans: Number of input channels (1 for SST)     embed_dim: Dimension of the patch embedding. The default is 128.         Many vision transformers use embedding dimensions that are multiples         of 64 (e.g., 64, 128, 256). This can be tuned.     patch_size: Tuple of (T, H, W) patch size. Default is (1, 4, 4).     drop: Probability of an element to be zeroed. Default is 0.0.         Increase it if there is overfitting.</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def __init__(self, in_chans=1, embed_dim=128, patch_size=(1, 4, 4), drop=0.0):\n    \"\"\"\n    Args:\n        in_chans: Number of input channels (1 for SST)\n        embed_dim: Dimension of the patch embedding. The default is 128.\n            Many vision transformers use embedding dimensions that are multiples\n            of 64 (e.g., 64, 128, 256). This can be tuned.\n        patch_size: Tuple of (T, H, W) patch size. Default is (1, 4, 4).\n        drop: Probability of an element to be zeroed. Default is 0.0.\n            Increase it if there is overfitting.\n    \"\"\"\n    super().__init__()\n    self.patch_size = patch_size\n\n    # proj is a Conv3d with kernel and stride = patch_size to create non-overlapping patches\n    # 2 * in_chans because we add a validity channel\n    self.proj = nn.Conv3d(\n        2 * in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n    )\n\n    # norm is LayerNorm over the embedding dimension to normalize patch embeddings\n    self.norm = nn.LayerNorm(embed_dim)\n\n    # dropout for regularization\n    self.drop = nn.Dropout(drop)\n</code></pre>"},{"location":"api/st_encoder_decoder/#climanet.st_encoder_decoder.VideoEncoder.forward","title":"<code>forward(x, mask)</code>","text":"<p>Forward pass with masking support via an additional validity channel. Args:     x: Input video tensor of shape (B, C, T, H, W)     mask: Boolean mask tensor of shape (B, C, T, H, W), where True     indicates masked pixels</p> <p>Returns:     Embedded patches of shape (B, N_patches, embed_dim)</p> <p>Notes:     - Masked pixels are zeroed out before patch embedding     - A validity mask (1 = observed, 0 = missing) is concatenated     as an additional input channel</p> Source code in <code>climanet/st_encoder_decoder.py</code> <pre><code>def forward(self, x, mask):\n    \"\"\"Forward pass with masking support via an additional validity channel.\n    Args:\n        x: Input video tensor of shape (B, C, T, H, W)\n        mask: Boolean mask tensor of shape (B, C, T, H, W), where True\n        indicates masked pixels\n\n    Returns:\n        Embedded patches of shape (B, N_patches, embed_dim)\n\n    Notes:\n        - Masked pixels are zeroed out before patch embedding\n        - A validity mask (1 = observed, 0 = missing) is concatenated\n        as an additional input channel\n    \"\"\"\n    # x: (B,1,T,H,W), mask: (B,1,T,H,W) where True means missing\n    valid = (~mask).float()\n    x = x * valid  # zero-out missing values\n    x = torch.cat([x, valid], dim=1)  # add validity as a channel\n    x = self.proj(x)  # (B, C, T', H', W')\n    x = rearrange(x, \"b c t h w -&gt; b (t h w) c\")\n    x = self.norm(x)\n    x = self.drop(x)\n    return x  # (B, N_patches, embed_dim)\n</code></pre>"},{"location":"api/utils/","title":"utils.py","text":""},{"location":"api/utils/#climanet.utils.add_month_day_dims","title":"<code>add_month_day_dims(daily_ts, monthly_ts, time_dim='time', spatial_dims=('lat', 'lon'))</code>","text":"<p>Reshape daily and monthly data to have explicit month (M) and day (T) dimensions.</p> <p>Here we assume maximum 31 days in a month, and invalid day entries will be padded with NaN.</p> <p>Returns:</p> Name Type Description <code>daily_m</code> <code>xr.DataArray - dims: (M, T, H, W)</code> <code>monthly_m</code> <code>xr.DataArray - dims: (M, H, W)</code> <code>padded_days_mask</code> <code>xr.DataArray - dims: (M, T=31), bool, True where day is padded</code> Source code in <code>climanet/utils.py</code> <pre><code>def add_month_day_dims(\n    daily_ts: xr.DataArray,    # (time, H, W) daily\n    monthly_ts: xr.DataArray,  # (time, H, W) monthly\n    time_dim: str = \"time\",\n    spatial_dims: Tuple[str, str] = (\"lat\", \"lon\")\n):\n    \"\"\" Reshape daily and monthly data to have explicit month (M) and day (T) dimensions.\n\n    Here we assume maximum 31 days in a month, and invalid day entries will be\n    padded with NaN.\n\n    Returns\n    -------\n    daily_m : xr.DataArray - dims: (M, T, H, W)\n    monthly_m : xr.DataArray - dims: (M, H, W)\n    padded_days_mask : xr.DataArray - dims: (M, T=31), bool, True where day is padded\n    \"\"\"\n    # Month key as integer YYYYMM\n    dkey = daily_ts[time_dim].dt.year * 100 + daily_ts[time_dim].dt.month\n    mkey = monthly_ts[time_dim].dt.year * 100 + monthly_ts[time_dim].dt.month\n\n    # Unique month keys preserving order\n    _, idx = np.unique(dkey.values, return_index=True)\n    month_keys = dkey.values[np.sort(idx)]\n\n    # Add M (month key) and T (day of month) coordinates to daily data\n    daily_indexed = (\n        daily_ts\n        .assign_coords(M=(time_dim, dkey.values), T=(time_dim, daily_ts[time_dim].dt.day.values))\n        .set_index({time_dim: (\"M\", \"T\")})\n        .unstack(time_dim)\n        .reindex(T=np.arange(1, 32), M=month_keys)\n    )\n    # Force dim order: (M, T, H, W) (and keep any other non-time dims after M,T)\n    other_dims = [d for d in daily_ts.dims if d != time_dim]  # e.g. [\"H\", \"W\"]\n    daily_indexed = daily_indexed.transpose(\"M\", \"T\", *other_dims)\n\n    # Build padded days mask from daily_indexed (NaN locations)\n    padded_days_mask = ~daily_indexed.notnull().any(dim=spatial_dims)\n\n    # Align monthly data to same month keys/order\n    monthly_m = (\n        monthly_ts\n        .assign_coords(M=(time_dim, mkey.values))\n        .swap_dims({time_dim: \"M\"})\n        .drop_vars(time_dim)\n        .sel(M=month_keys)\n    )\n\n    return daily_indexed, monthly_m, padded_days_mask\n</code></pre>"},{"location":"api/utils/#climanet.utils.pred_to_numpy","title":"<code>pred_to_numpy(pred, orig_H=None, orig_W=None, land_mask=None)</code>","text":"<p>pred: (B, M, H_pad,W_pad) or (B, H, W) torch tensor orig_H/W: original sizes before padding (optional) land_mask: (H_pad,W_pad) or (H,W) bool; if given, land will be set to NaN returns: (H,W) numpy array</p> Source code in <code>climanet/utils.py</code> <pre><code>def pred_to_numpy(pred, orig_H=None, orig_W=None, land_mask=None):\n    \"\"\"\n    pred: (B, M, H_pad,W_pad) or (B, H, W) torch tensor\n    orig_H/W: original sizes before padding (optional)\n    land_mask: (H_pad,W_pad) or (H,W) bool; if given, land will be set to NaN\n    returns: (H,W) numpy array\n    \"\"\"\n    # crop to original size if provided\n    if orig_H is not None and orig_W is not None:\n        pred = pred[..., :orig_H, :orig_W]\n        if land_mask is not None:\n            land_mask = land_mask[..., :orig_H, :orig_W]\n\n    # set land to NaN (broadcast mask across batch)\n    if land_mask is not None:\n        pred = pred.clone().to(torch.float32)\n        pred[:, :, land_mask.bool()] = float(\"nan\")\n\n    return pred.detach().cpu().numpy()\n</code></pre>"},{"location":"api/utils/#climanet.utils.regrid_to_boundary_centered_grid","title":"<code>regrid_to_boundary_centered_grid(da, roll=False)</code>","text":"<p>Interpolates a DataArray from its current center-based grid onto a new grid whose coordinates are derived from user-specified boundaries.</p> <p>Includes robust handling for 0-360 vs -180-180 longitude domains.</p> <p>Assumes dimensions are named 'lat' and 'lon'.</p> Source code in <code>climanet/utils.py</code> <pre><code>def regrid_to_boundary_centered_grid(\n    da: xr.DataArray,\n    roll = False\n) -&gt; xr.DataArray:\n    \"\"\"\n    Interpolates a DataArray from its current center-based grid onto a new\n    grid whose coordinates are derived from user-specified boundaries.\n\n    Includes robust handling for 0-360 vs -180-180 longitude domains.\n\n    Assumes dimensions are named 'lat' and 'lon'.\n    \"\"\"\n    print(\"Starting regridding process...\")\n\n    # --- 0. Longitude Domain Check and Correction ---\n\n    input_lon = da['longitude']\n\n    # Check if roll for 0-360 to -180-180 is requested\n    if roll:\n        print(\"Applying cyclic roll to -180 to 180...\")\n\n        # Calculate the index closest to 180 degrees\n        lon_diff = np.abs(input_lon - 180.0)\n        # We need to roll such that the 180-degree line is moved to the edge\n        # and the new array starts near -180\n        roll_amount = int(lon_diff.argmin().item() + (input_lon.size / 2)) % input_lon.size\n\n        # Roll the DataArray and its coordinates\n        da = da.roll(longitude=roll_amount, roll_coords=True)\n\n        # Correct the longitude coordinate values: shift values &gt; 180 down by 360\n        new_lon_coords = da['longitude'].where(da['longitude'] &lt;= 180, da['longitude'] - 360)\n\n        # Assign the corrected and sorted coordinates\n        da = da.assign_coords(longitude=new_lon_coords).sortby('longitude')\n        print(f\"Longitudes adjusted. New range: {da['longitude'].min().item():.2f} \"\n              f\"to {da['longitude'].max().item():.2f}\")\n\n    # --- 1. Define Target Grid Boundaries ---\n\n    # Target latitude boundaries: -90.0 up to 90.0 in 0.25 degree steps\n    # (721 points)\n    lat_bnds = np.linspace(-90.0, 90.0, 721)\n\n    # Target longitude boundaries: -180.0 up to 180.0 in 0.25 degree steps\n    # (1441 points)\n    lon_bnds = np.linspace(-180.0, 180.0, 1441)\n\n    # --- 2. Calculate New Grid Centers (Coordinates) ---\n\n    # New latitude centers are the average of consecutive boundaries\n    # (720 points)\n    new_lats = (lat_bnds[:-1] + lat_bnds[1:]) / 2.0\n\n    # New longitude centers are the average of consecutive boundaries\n    # (1440 points)\n    new_lons = (lon_bnds[:-1] + lon_bnds[1:]) / 2.0\n\n    # --- 3. Interpolate the Data ---\n\n    # Use linear interpolation (suitable for gappy data) to map data onto the\n    # new centers. xarray handles the NaNs automatically.\n    da_regridded = da.interp(\n        latitude=new_lats,\n        longitude=new_lons,\n        method=\"linear\"\n    )\n\n    print(f\"Regridding complete. New dimensions: {da_regridded.dims}\")\n    return da_regridded\n</code></pre>"}]}