
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Spatio Temporal Model (class SpatioTemporalModel) - ClimaNet</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="white">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#spatio-temporal-model-class-spatiotemporalmodel" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ClimaNet" class="md-header__button md-logo" aria-label="ClimaNet" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ClimaNet
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Spatio Temporal Model (class SpatioTemporalModel)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="white"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/ESMValGroup/ClimaNet/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ClimaNet
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Introduction

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../examples/" class="md-tabs__link">
        
  
  
    
  
  Examples

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../api/st_encoder_decoder/" class="md-tabs__link">
          
  
  
  API Reference

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ClimaNet" class="md-nav__button md-logo" aria-label="ClimaNet" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    ClimaNet
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ESMValGroup/ClimaNet/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    ClimaNet
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Introduction
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Examples
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    API Reference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    API Reference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/st_encoder_decoder/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    st_encoder_decoder.py
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../api/utils/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    utils.py
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-video-encoder-class-videoencoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Video Encoder (class VideoEncoder)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-temporal-positional-encoding-class-temporalpositionalencoding" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Temporal positional encoding (class TemporalPositionalEncoding)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-temporal-attention-aggregator-class-temporalattentionaggregator" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Temporal Attention Aggregator (class TemporalAttentionAggregator)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-spatial-positional-encoding-class-spatialpositionalencoding2d" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Spatial Positional Encoding (class SpatialPositionalEncoding2D)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-spatial-transformer-class-spatialtransformer" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Spatial Transformer (class SpatialTransformer)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-monthly-convolution-decoder-class-monthlyconvdecoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Monthly Convolution Decoder (class MonthlyConvDecoder)
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="spatio-temporal-model-class-spatiotemporalmodel">Spatio Temporal Model (class <code>SpatioTemporalModel</code>)</h1>
<p><strong>Summary:</strong>
- Combines video encoder, temporal attention, spatial transformer, and decoder
- Encodes video into spatio-temporal patches
- Aggregates temporal information per spatial patch
- Mixes spatial features across patches
- Decodes back to original spatial resolution</p>
<p><strong>Detailed process:</strong></p>
<p>The model takes daily SST (or similar) data in video format: <code>x ∈ ℝ^{B × 1 × T ×
H × W}</code> and a <code>daily_mask</code> indicating missing pixels. It also takes
<code>land_mask_patch</code> indicating land regions in the output.</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="c1"># 1. Patch embedding:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>X<span class="w"> </span><span class="o">(</span>VideoEncoder<span class="o">)</span>---------&gt;<span class="w"> </span>X_patch
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="c1"># 2. Add temporal encoding +</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="c1"># 3. Temporal aggregation:</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>X_patch<span class="w"> </span>+<span class="w"> </span>PE<span class="w"> </span><span class="o">(</span>TemporalAttentionAggregator<span class="o">)</span>---------&gt;<span class="w"> </span>X_temp_agg
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># 4. Add spatial encoding +</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="c1"># 5. Spatial transformer:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>X_temp_agg<span class="w"> </span>+<span class="w"> </span>PE<span class="w"> </span><span class="o">(</span>SpatialTransformer<span class="o">)</span><span class="w"> </span>---------&gt;<span class="w"> </span>X_mixed
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="c1"># 6. Decode to original resolution:</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>X_mixed<span class="w"> </span><span class="o">(</span>MonthlyConvDecoder<span class="o">)</span>---------&gt;<span class="w"> </span>Output
</code></pre></div>
<h2 id="1-video-encoder-class-videoencoder">1. Video Encoder (class <code>VideoEncoder</code>)</h2>
<p><strong>Summary:</strong></p>
<ul>
<li>Masked pixels are removed but their locations are preserved</li>
<li>Video is split into 3D patches (time × height × width)</li>
<li>Each patch becomes a vector (embedding)</li>
<li>Output is a sequence suitable for Transformer-based video models (e.g. VideoMAE)</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>We start with a video: Input video <code>x ∈ R^{B × 1 × T × H × W}</code> (batch, channel,
time, height, width) and mask ∈ <code>{0,1}^{B × 1 × T × H × W}</code> where 1 (True) means
missing / masked at ocean pixels. We define a validity indicator: <code>valid = 1 −
mask</code>. So: valid = 1 → observed pixel, valid = 0 → missing pixel. We zero out
missing values: <code>x_valid = x ⊙ valid</code>.</p>
<p>We then concatenate the validity mask as a second channel: <code>x_cat =
concat(x_valid, valid)</code>. Now the input has 2 channels: <code>x_cat ∈ R^{B × 2 × T × H
× W}</code>. This allows the model to know which values were observed and which were
missing.</p>
<p>We split the video into non-overlapping spatio-temporal patches using a 3D
convolution, see
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html">torch.nn.Conv3d</a>.
Let the patch size be: <code>(Pt, Ph, Pw)</code>. The convolution uses: <code>kernel size = (Pt,
Ph, Pw)</code>, <code>stride = (Pt, Ph, Pw)</code>. This means each convolution output
corresponds to one patch and patches do not overlap. Resulting shape: <code>z ∈ R^{B
× D × T' × H' × W'}</code> where: <code>D = embed_dim</code>, <code>T' = T / Pt, H' = H / Ph, W' = W /
Pw</code>. Each <code>(t', h', w')</code> location is a patch embedding vector of length D.</p>
<p>We flatten the 3D grid of patches into a sequence: <code>N = T' × H' × W'</code>. So each
video becomes a sequence of patch embeddings, just like tokens in a Transformer.</p>
<p>For each patch embedding, Layer normalization is done. This stabilizes training
by normalizing across the embedding dimension, see
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">torch.nn.LayerNorm</a>.</p>
<p>Randomly drops elements for regularization is done with Dropout. This helps
prevent overfitting during training, see
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html">torch.nn.Dropout</a>.</p>
<p>The final output is: <code>{B × N_patches × embed_dim}</code>. Each element
represents a spatio-temporal video patch, enriched with: visual information,
knowledge of which pixels were valid or missing.</p>
<h2 id="2-temporal-positional-encoding-class-temporalpositionalencoding">2. Temporal positional encoding (class <code>TemporalPositionalEncoding</code>)</h2>
<p><strong>Summary:</strong></p>
<ul>
<li>Each time step gets a unique vector</li>
<li>Encodings are deterministic and fixed</li>
<li>No learnable parameters</li>
<li>Based on the Transformer positional encoding design</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>The purpose of temporal positional encoding is to generate fixed temporal
position vectors so that a model can know at which time index a feature occurs.
The encoding depends only on time index, not on the data.</p>
<p>Assume a temporal sequence of length: <code>T = 0, 1, 2, ..., T−1</code>. Each time index <code>t</code>
is assigned a vector of length embedding dim. For time index <code>t</code> and
embedding dimension index <code>i</code>: Even dimensions use sine, odd dimensions use
cosine. This produces a unique, smooth encoding for each time step.</p>
<p>For a maximum supported temporal length <code>max_len</code>, the class precomputes <code>pe</code>
where row <code>t</code> contains the encoding for time index <code>t</code>. This matrix is fixed,
not trainable and stored as a buffer. Later, in <code>forward</code> method, given a
requested temporal length <code>T</code>, we have <code>output = PE[0:T]</code> and resulting shape is
<code>(T, embed_dim)</code>. No parameters are learned and no computation depends on input
data.</p>
<h2 id="3-temporal-attention-aggregator-class-temporalattentionaggregator">3. Temporal Attention Aggregator (class <code>TemporalAttentionAggregator</code>)</h2>
<p>This is a temporal attention pooling over sequences of tokens.</p>
<p><strong>Summary:</strong></p>
<p>For each spatial patch (h, w):</p>
<ul>
<li>Collect its T temporal tokens: each spatial patch (h, w) has T temporal tokens</li>
<li>Add temporal positional encoding for days within a month and for months within a year.</li>
<li>Compute a learned scalar score per time step</li>
<li>Apply softmax over time: it ensures the weights form a probability
  distribution over time.</li>
<li>Compute a weighted temporal sum: output is a temporal summary vector for each
  patch, suitable for downstream tasks.</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>We start with: <code>x ∈ ℝ^{B × (M.T·H·W) × C}</code>, where <code>B</code> = batch size, <code>M</code>= number
of months, <code>T</code> = number of temporal tokens per spatial patch, <code>H</code>, <code>W</code> = number
of spatial patches along height and width, <code>C</code> = embedding dimension. We can
reshape it to group by spatial patch where each spatial patch has its temporal
sequence of length <code>T</code>.</p>
<p>Then we add temporal positional encoding <code>pe</code> from <code>TemporalPositionalEncoding</code>.
Add it to each temporal token <code>seq = seq + pe</code>. This injects time information (for days and for months) into each patch’s token sequence.</p>
<p>Then we compute temporal attention weights by applying <code>nn.Sequential</code> to get a
scalar score, see
<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html">torch.nn.Sequential</a>.
Here the explanation over each module in the sequential is as follows:</p>
<ul>
<li><code>LayerNorm</code> normalizes the features across the embedding dimension, which
  helps stabilize training. This is a common practice before attention
  computation.</li>
<li><code>Linear(embed_dim, embed_dim)</code> learns which features are important for temporal weighting.</li>
<li><code>GELU</code> allows learning non-linear relationships.</li>
<li><code>Linear(embed_dim, 1)</code> projects to a single scalar score.</li>
</ul>
<p>see <a href="https://docs.pytorch.org/docs/stable/nn.html">torch.nn</a> for more details on
each module.</p>
<p>We apply the mask <code>padded_days_mask</code> where padded days (beyond the actual number
of days in a month) are masked out. This ensures that the model does not attend
to padded tokens that do not correspond to real data.Then, we convert scores
into attention weights using softmax over time that represents importance of
each temporal token for this patch. Then we aggregate temporal tokens by
weighted sum over the temporal dimension. Result is one token per spatial patch.</p>
<p>Then we apply cross month mixing using <code>nn.MultiheadAttention</code>. This allows
tokens from different months to attend to each other, which can capture seasonal patterns. The attention is computed across the temporal dimension for each spatial patch. See <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#multiheadattention">torch.nn.MultiheadAttention</a> for details on how multi-head attention works.</p>
<p>Then we apply <code>nn.Sequential</code> again to the output of attention to get a final scalar score for each time step. The explanation for each module is the same as before: LayerNorm, Linear, GELU, Linear to 1 scalar. This gives us a score for each time step in the temporal sequence of each spatial patch.</p>
<h2 id="4-spatial-positional-encoding-class-spatialpositionalencoding2d">4. Spatial Positional Encoding (class <code>SpatialPositionalEncoding2D</code>)</h2>
<p><strong>Summary:</strong></p>
<ul>
<li>Generate fixed sinusoidal positional encodings for 2D spatial grid</li>
<li>Encodings are not learnable</li>
<li>Intended to be added to spatial tokens</li>
<li>Sine and cosine functions of different frequencies allow the model to
  distinguish positions along height and width.</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>The module generates fixed 2D positional encodings for a grid of size (H, W) and
embedding dimension embed_dim. Each spatial location (h, w) gets a unique vector
of length embed_dim. Encodings are deterministic (not learned) and based on
sine/cosine functions, similar to the <code>Temporal positional encoding</code>. The
encoding for each spatial location is a combination of sine and cosine functions
of different frequencies along height and width. This allows the model to know
the spatial position of each token when added to the spatial tokens.</p>
<h2 id="5-spatial-transformer-class-spatialtransformer">5. Spatial Transformer (class <code>SpatialTransformer</code>)</h2>
<p>It mixes spatial patch tokens using multi-head self-attention.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>Applies a standard Transformer encoder to spatial tokens</li>
<li>Mixes information across spatial locations: each patch token can attend to all
  other patches, allowing global spatial context</li>
<li>Output is a sequence of spatially mixed tokens of the same shape as input</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>We apply a single Transformer encoder block using <code>nn.TransformerEncoderLayer</code>
that performs self-attention and feedforward processing. In Multi-Head
Self-Attention, Every token looks at every other token and with <code>num_heads=4</code>,
this happens in 4 parallel "perspectives." In Feedforward Network (MLP), each
token is processed independently through a small neural network to allow complex
feature interactions. This is useful for spatial data and allows:</p>
<ul>
<li>Global context: Every patch can "see" every other patch</li>
<li>Spatial mixing: Information flows across the entire image</li>
<li>Learning relationships: Model learns which patches are relevant to each other</li>
</ul>
<p>Then, <code>nn.TransformerEncoder</code> stacks multiple encoder layers sequentially; it's
a container that repeats the same transformer block <code>depth</code> times.</p>
<h2 id="6-monthly-convolution-decoder-class-monthlyconvdecoder">6. Monthly Convolution Decoder (class <code>MonthlyConvDecoder</code>)</h2>
<p><strong>Summary:</strong></p>
<ul>
<li>Reshape latent tokens</li>
<li>Apply 1x1 convolution to mix features</li>
<li>Use transposed convolution to upsample to original spatial size</li>
<li>Applies a convolutional refinement block to smooth patch boundaries.</li>
<li>Apply convolutional head for final output</li>
<li>Optionally mask out land regions</li>
<li>Add scale and bias to output</li>
</ul>
<p><strong>Detailed process:</strong></p>
<p>We transforms the embedding dimension (default embed_dim=128) to the hidden
dimension (default hidden=128) at each spatial location independently by a 1×1
convolution (also called a "pointwise convolution") using <code>nn.Conv2d(...,
kernel_size=1)</code>. Even though both input and output dimensions are 128 by
default, this layer learns a linear transformation to mix and re-weight the
channel features.</p>
<p>Then we use a deconvolution (<code>ConvTranspose2d</code>) to map each patch to its
original pixel grid. It converts the low-resolution patch grid back to the
original image resolution. The padding is set to the overlap, which allows for
smooth upsampling and some overlap between patches.</p>
<p>Then we apply a convolutional refinement block to smooth out any artifacts from
the deconvolution. Deconvolution can sometimes produce blocky outputs, so this
refinement helps to smooth patch boundaries and improve spatial coherence. This
block consists of a <code>nn.Sequential</code>. This processing block contains:</p>
<ul>
<li><code>Conv2d</code>: Convolution to mix features and reduce artifacts.</li>
<li><code>BatchNorm2d</code>: Normalizes the features to stabilize training.</li>
<li><code>GELU</code>: Non-linear activation to allow complex feature interactions.</li>
<li><code>Conv2d(1×1)</code>: Final 1×1 convolution for deeper feature refinement.</li>
<li><code>BatchNorm2d</code>: Re-normalizes the features to stabilize training.</li>
<li><code>GELU</code>: Another non-linear activation to allow complex feature interactions.</li>
</ul>
<p>A single <code>Conv2d(..., 1, kernel_size=1)</code> could work, but the extra layers provide:</p>
<ul>
<li>Spatial refinement after deconvolution (which can produce artifacts)</li>
<li>Non-linearity for more expressive power</li>
<li>Better gradient flow during training</li>
</ul>
<p>Finally, we apply a small convolutional head to produce the final single-channel
output. Then we apply scale, bias, and (optional) land mask to get the final
output (he reconstructed 2D map (e.g., SST) for each batch).</p>
<h2 id="references">References</h2>
<ul>
<li><a href="https://doi.org/10.48550/arXiv.1706.03762">Attention is all you need</a></li>
<li><a href="https://doi.org/10.48550/arXiv.2203.12602">VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training</a></li>
<li><a href="https://doi.org/10.48550/arXiv.2205.09113">Masked Autoencoders As Spatiotemporal Learners</a></li>
<li><a href="https://doi.org/10.5194/os-20-1309-2024">MAESSTRO: Masked Autoencoders for Sea Surface Temperature Reconstruction under Occlusion- 2024</a></li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "content.code.copy", "content.tabs"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>